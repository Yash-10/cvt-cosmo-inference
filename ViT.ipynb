{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for training a ViT:\n",
    "\n",
    "- https://wandb.ai/dtamkus/posts/reports/5-Tips-for-Creating-Lightweight-Vision-Transformers--Vmlldzo0MjQyMzg0\n",
    "- Improving performance of ViTs on small datasets: https://keras.io/examples/vision/vit_small_ds/\n",
    "- What if we use a pretrained ViT from Imagenet, for example? See [here](https://github.com/hananshafi/vits-for-small-scale-datasets):\n",
    "> ...in contrast to convolutional neural networks, Vision Transformer lacks inherent inductive biases. Therefore, successful training of such models is mainly attributed to pre-training on large-scale datasets such as ImageNet with 1.2M or JFT with 300M images.\n",
    "- The self-attention mechanism allows to learn the relationship between different patches in the input sequence. From [here](https://arxiv.org/pdf/2304.08192.pdf).\n",
    "- ViTs: permutation-equivariant, not translation invariant, not feature heirarchy. See [the tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6_Dfu_LC1be"
   },
   "outputs": [],
   "source": [
    "!pip install Pylians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qukuyJGwC6Vr"
   },
   "outputs": [],
   "source": [
    "!pip install torch_intermediate_layer_getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6SSUy29CyeW"
   },
   "outputs": [],
   "source": [
    "USE_COLAB = False\n",
    "if USE_COLAB:\n",
    "    base_dir = '/content'\n",
    "else:\n",
    "    base_dir = '/kaggle/working'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:31:47.088175Z",
     "iopub.status.busy": "2024-02-15T05:31:47.087866Z",
     "iopub.status.idle": "2024-02-15T05:31:54.519007Z",
     "shell.execute_reply": "2024-02-15T05:31:54.517821Z",
     "shell.execute_reply.started": "2024-02-15T05:31:47.088149Z"
    },
    "id": "G3D4sMBXCzpp"
   },
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    !rm -rf /content/ViT-LSS\n",
    "else:\n",
    "    !rm -rf /kaggle/working/ViT-LSS\n",
    "!git clone https://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@github.com/Yash-10/ViT-LSS.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:31:54.520676Z",
     "iopub.status.busy": "2024-02-15T05:31:54.520364Z",
     "iopub.status.idle": "2024-02-15T05:31:55.549134Z",
     "shell.execute_reply": "2024-02-15T05:31:55.548042Z",
     "shell.execute_reply.started": "2024-02-15T05:31:54.520648Z"
    },
    "id": "HAN3xzHkC27q"
   },
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    !cp /content/ViT-LSS/scripts/*.py /content\n",
    "else:\n",
    "    !cp /kaggle/working/ViT-LSS/scripts/*.py /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:31:55.550796Z",
     "iopub.status.busy": "2024-02-15T05:31:55.550484Z",
     "iopub.status.idle": "2024-02-15T05:32:09.931988Z",
     "shell.execute_reply": "2024-02-15T05:32:09.930833Z",
     "shell.execute_reply.started": "2024-02-15T05:31:55.550768Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning==2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:32:09.933775Z",
     "iopub.status.busy": "2024-02-15T05:32:09.933460Z",
     "iopub.status.idle": "2024-02-15T05:32:20.931002Z",
     "shell.execute_reply": "2024-02-15T05:32:20.929885Z",
     "shell.execute_reply.started": "2024-02-15T05:32:09.933748Z"
    }
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"./\"\n",
    "\n",
    "# Setting the seed\n",
    "SEED = 42\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:33:12.213076Z",
     "iopub.status.busy": "2024-02-15T05:33:12.212502Z",
     "iopub.status.idle": "2024-02-15T05:33:12.222625Z",
     "shell.execute_reply": "2024-02-15T05:33:12.221640Z",
     "shell.execute_reply.started": "2024-02-15T05:33:12.213044Z"
    },
    "id": "KYPHM0ksCvoy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "if USE_COLAB:\n",
    "    from google.colab import drive\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import time, sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# optimizer parameters\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# TODO: Need to do hyperparameter optimization.\n",
    "batch_size = 32\n",
    "lr         = 1e-3\n",
    "wd         = 1e-5  #value of weight decay\n",
    "dr         = 0.2    #dropout value for fully connected layers\n",
    "epochs     = 25    #number of epochs to train the network\n",
    "\n",
    "channels        = 1                #we only consider here 1 field\n",
    "params          = [0,1,2,3,4]    #Omega_m, Omega_b, h, n_s, sigma_8. The code will be trained to predict all these parameters.\n",
    "g               = params           #g will contain the mean of the posterior\n",
    "h               = [5+i for i in g] #h will contain the variance of the posterior\n",
    "\n",
    "model_kwargs = {\n",
    "    'embed_dim': 256,\n",
    "    'hidden_dim': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'patch_size': 16,\n",
    "    'num_channels': 1,\n",
    "    'num_patches': 16,\n",
    "    'num_classes': 10,\n",
    "    'dropout': dr\n",
    "}\n",
    "\n",
    "GRID_SIZE = 64\n",
    "\n",
    "# output files names\n",
    "floss  = 'loss.txt'   #file with the training and validation losses for each epoch\n",
    "fmodel = 'weights.pt' #file containing the weights of the best-model\n",
    "\n",
    "num_maps_per_projection_direction = 10\n",
    "num_sims = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:33:12.430458Z",
     "iopub.status.busy": "2024-02-15T05:33:12.429725Z",
     "iopub.status.idle": "2024-02-15T05:33:12.435863Z",
     "shell.execute_reply": "2024-02-15T05:33:12.434974Z",
     "shell.execute_reply.started": "2024-02-15T05:33:12.430427Z"
    },
    "id": "I-rT7qOtC8mA"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(style='ticks')\n",
    "sns.set_context(\"paper\", font_scale = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of fields for different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:33:13.833913Z",
     "iopub.status.busy": "2024-02-15T05:33:13.833171Z",
     "iopub.status.idle": "2024-02-15T05:34:06.328205Z",
     "shell.execute_reply": "2024-02-15T05:34:06.327221Z",
     "shell.execute_reply.started": "2024-02-15T05:33:13.833884Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import read_hdf5\n",
    "import glob\n",
    "\n",
    "o, s = [], []\n",
    "sorted_files = sorted(glob.glob('/kaggle/input/density-fields-vit-lss-64/my_outputs/*.h5'))\n",
    "for f in sorted_files:\n",
    "    _, params = read_hdf5(f)\n",
    "    o.append(params[0])\n",
    "    s.append(params[4])  # 4 or -1 will give the same.\n",
    "\n",
    "den_max_om, den_max_om_params = read_hdf5(sorted_files[o.index(max(o))])\n",
    "den_min_om, den_min_om_params = read_hdf5(sorted_files[o.index(min(o))])\n",
    "den_max_s8, den_max_s8_params = read_hdf5(sorted_files[s.index(max(s))])\n",
    "den_min_s8, den_min_s8_params = read_hdf5(sorted_files[s.index(min(s))])\n",
    "\n",
    "# Showing the same (random) slice from all four cases.\n",
    "vmin = np.log10(min(den_max_om[:, 10, :].min(), den_min_om[:, 10, :].min(), den_max_s8[:, 10, :].min(), den_min_s8[:, 10, :].min()))\n",
    "vmax = np.log10(max(den_max_om[:, 10, :].max(), den_min_om[:, 10, :].max(), den_max_s8[:, 10, :].max(), den_min_s8[:, 10, :].max()))\n",
    "plt.imshow(np.log10(den_max_om[:, 10, :]), vmin=vmin, vmax=vmax); plt.title(den_max_om_params); plt.colorbar(); plt.show()\n",
    "plt.imshow(np.log10(den_min_om[:, 10, :]), vmin=vmin, vmax=vmax); plt.title(den_min_om_params); plt.colorbar(); plt.show()\n",
    "plt.imshow(np.log10(den_max_s8[:, 10, :]), vmin=vmin, vmax=vmax); plt.title(den_max_s8_params); plt.colorbar(); plt.show()\n",
    "plt.imshow(np.log10(den_min_s8[:, 10, :]), vmin=vmin, vmax=vmax); plt.title(den_min_s8_params); plt.colorbar(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4t6K6JpDkkP"
   },
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:34:06.330672Z",
     "iopub.status.busy": "2024-02-15T05:34:06.330026Z",
     "iopub.status.idle": "2024-02-15T05:34:06.335758Z",
     "shell.execute_reply": "2024-02-15T05:34:06.334790Z",
     "shell.execute_reply.started": "2024-02-15T05:34:06.330643Z"
    },
    "id": "8U1mGAqaDmMA"
   },
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    !wget https://www.dropbox.com/scl/fi/jqyvpxl17hp7pinqtd68c/density_fields_3D_LH_z0_grid64_masCIC.tar.gz?rlkey=cvf3oxbd922xxrzv8tue0zoiv&dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:34:06.337518Z",
     "iopub.status.busy": "2024-02-15T05:34:06.337144Z",
     "iopub.status.idle": "2024-02-15T05:34:06.355659Z",
     "shell.execute_reply": "2024-02-15T05:34:06.354898Z",
     "shell.execute_reply.started": "2024-02-15T05:34:06.337485Z"
    },
    "id": "_FCqvYIWDoKb"
   },
   "outputs": [],
   "source": [
    "if USE_COLAB:\n",
    "    !tar -xzf /content/density_fields_3D_LH_z0_grid64_masCIC.tar.gz?rlkey=cvf3oxbd922xxrzv8tue0zoiv\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:40:05.976281Z",
     "iopub.status.busy": "2024-02-15T05:40:05.975404Z",
     "iopub.status.idle": "2024-02-15T05:40:05.980810Z",
     "shell.execute_reply": "2024-02-15T05:40:05.979960Z",
     "shell.execute_reply.started": "2024-02-15T05:40:05.976249Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "prefix=''\n",
    "if USE_COLAB:\n",
    "    command = [\n",
    "        'python', 'create_data.py', '--num_sims', f'{num_sims}', '--train_frac', '0.8', '--test_frac', '0.1',\n",
    "        '--seed', f'{SEED}', '--path', '/content/my_outputs', '--grid_size', f'{GRID_SIZE}',\n",
    "        '--num_maps_per_projection_direction', f'{num_maps_per_projection_direction}', '--prefix', ''\n",
    "    ]\n",
    "else:\n",
    "    command = [\n",
    "        'python', 'create_data.py', '--num_sims', f'{num_sims}', '--train_frac', '0.8', '--test_frac', '0.1',\n",
    "        '--seed', f'{SEED}', '--path', '/kaggle/input/density-fields-vit-lss-64/my_outputs', '--grid_size', f'{GRID_SIZE}',\n",
    "        '--num_maps_per_projection_direction', f'{num_maps_per_projection_direction}', '--prefix', ''\n",
    "    ]\n",
    "result = subprocess.run(command)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:40:10.418015Z",
     "iopub.status.busy": "2024-02-15T05:40:10.417256Z",
     "iopub.status.idle": "2024-02-15T05:40:10.425797Z",
     "shell.execute_reply": "2024-02-15T05:40:10.424849Z",
     "shell.execute_reply.started": "2024-02-15T05:40:10.417979Z"
    },
    "id": "6B6CLSCvDsAG"
   },
   "outputs": [],
   "source": [
    "# Store the mean, std, min_vals and max_vals into variables\n",
    "MEAN = np.load(f'{prefix}_dataset_mean.npy')\n",
    "STD = np.load(f'{prefix}_dataset_std.npy')\n",
    "MIN_VALS = np.load(f'{prefix}_dataset_min_vals.npy')\n",
    "MAX_VALS = np.load(f'{prefix}_dataset_max_vals.npy')\n",
    "MEAN_DENSITIES = np.load(f'{prefix}_dataset_mean_densities.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:40:11.371355Z",
     "iopub.status.busy": "2024-02-15T05:40:11.370504Z",
     "iopub.status.idle": "2024-02-15T05:40:14.360696Z",
     "shell.execute_reply": "2024-02-15T05:40:14.359378Z",
     "shell.execute_reply.started": "2024-02-15T05:40:11.371324Z"
    },
    "id": "DFjuoSrRDtuU"
   },
   "outputs": [],
   "source": [
    "!ls train | wc -l\n",
    "!ls val | wc -l\n",
    "!ls test | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:40:15.840701Z",
     "iopub.status.busy": "2024-02-15T05:40:15.840283Z",
     "iopub.status.idle": "2024-02-15T05:40:16.010590Z",
     "shell.execute_reply": "2024-02-15T05:40:16.009496Z",
     "shell.execute_reply.started": "2024-02-15T05:40:15.840665Z"
    },
    "id": "kPo7aNZ1D0v2"
   },
   "outputs": [],
   "source": [
    "from model_dataset import CustomImageDataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import random\n",
    "# See https://pytorch.org/vision/0.15/transforms.html\n",
    "class MyRotationTransform:\n",
    "    \"\"\"Rotate by 90/180/270 degrees.\"\"\"\n",
    "\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        x = torch.from_numpy(x).unsqueeze(0)\n",
    "        return TF.rotate(x, angle).squeeze()\n",
    "\n",
    "transform = v2.Compose([\n",
    "    MyRotationTransform(angles=[90, 180, 270]),\n",
    "    v2.ToDtype(torch.float32),#, scale=False),\n",
    "])\n",
    "\n",
    "train_dataset = CustomImageDataset(f'{base_dir}/train', normalized_cosmo_params_path='train/train_normalized_params.csv', transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomImageDataset(f'{base_dir}/val', normalized_cosmo_params_path='val/val_normalized_params.csv', transform=None)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = CustomImageDataset(f'{base_dir}/test', normalized_cosmo_params_path='test/test_normalized_params.csv', transform=None)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:40:17.076902Z",
     "iopub.status.busy": "2024-02-15T05:40:17.076545Z",
     "iopub.status.idle": "2024-02-15T05:40:17.881898Z",
     "shell.execute_reply": "2024-02-15T05:40:17.880887Z",
     "shell.execute_reply.started": "2024-02-15T05:40:17.076875Z"
    }
   },
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
    "    return x\n",
    "\n",
    "x = next(iter(train_loader))\n",
    "print(x[0].shape)\n",
    "\n",
    "# See original code in the original tutorial: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html\n",
    "img_patches = img_to_patch(x[0], patch_size=16, flatten_channels=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "fig.suptitle(\"Images as input sequences of patches\")\n",
    "for i in range(x[0].shape[0]):\n",
    "    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=4, normalize=False, pad_value=0.9)\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "    ax.imshow(img_grid)\n",
    "    ax.axis('off')\n",
    "    break\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Remember that the positional embedding (`self.pos_embedding`) below is learnable assuming we use fixed-resolution images. In the future, we would like to transfer learn to different resolution images, in which case we need to use the sine and cosine functions proposed in the original ViT paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:40:20.957088Z",
     "iopub.status.busy": "2024-02-15T05:40:20.956685Z",
     "iopub.status.idle": "2024-02-15T05:40:20.961638Z",
     "shell.execute_reply": "2024-02-15T05:40:20.960694Z",
     "shell.execute_reply.started": "2024-02-15T05:40:20.957056Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import get_rmse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:40:22.125006Z",
     "iopub.status.busy": "2024-02-15T05:40:22.124289Z",
     "iopub.status.idle": "2024-02-15T05:40:22.156488Z",
     "shell.execute_reply": "2024-02-15T05:40:22.155641Z",
     "shell.execute_reply.started": "2024-02-15T05:40:22.124958Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform regression prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ViT(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_kwargs, lr, wd, beta1, beta2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = VisionTransformer(**model_kwargs)\n",
    "        self.example_input_array = next(iter(train_loader))[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # enforce the errors to be positive\n",
    "        y = torch.clone(x)\n",
    "        y[:,5:10] = torch.square(x[:,5:10])\n",
    "        return y\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.wd, betas=(self.hparams.beta1, self.hparams.beta2))\n",
    "        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=5)\n",
    "#         return [optimizer], [lr_scheduler]\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        x, y, _ = batch\n",
    "        p = self.model(x)\n",
    "        y_NN = p[:,g]             #posterior mean\n",
    "        e_NN = p[:,h]             #posterior std\n",
    "        # TODO: Ensure this below implementation of loss is actually the one optimized in CAMELS.\n",
    "        loss1 = torch.mean((y_NN - y)**2,                axis=0)\n",
    "        loss2 = torch.mean(((y_NN - y)**2 - e_NN**2)**2, axis=0)\n",
    "        loss  = torch.mean(torch.log(loss1) + torch.log(loss2))\n",
    "        self.log(f'{mode}_loss', loss)\n",
    "\n",
    "        # TODO: Where are the logs/logged values stored? Need to find and print manually at end of training outside this function.\n",
    "\n",
    "        # Also log RMSE and sigma_bar for all parameters.\n",
    "        rmse = get_rmse_score(y.cpu().detach().numpy(), y_NN.cpu().detach().numpy())\n",
    "        sigma_bar = np.mean(y_NN.cpu().detach().numpy(), axis=0)\n",
    "        # Only log at the end of epoch instead of each step.\n",
    "        # Logging is only done for Omega_m and sigma_8 since only these are interesting for DM density/DM halo fields.\n",
    "        # But more can easily be added here if and when needed.\n",
    "        self.log(f'{mode}_omegam_rmse', rmse[0], on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_omegam_sigma_bar', sigma_bar[0], on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_sigma8_rmse', rmse[-1], on_step=False, on_epoch=True)\n",
    "        self.log(f'{mode}_sigma8_sigma_bar', sigma_bar[-1], on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:40:23.250768Z",
     "iopub.status.busy": "2024-02-15T05:40:23.250029Z",
     "iopub.status.idle": "2024-02-15T05:40:23.259874Z",
     "shell.execute_reply": "2024-02-15T05:40:23.258887Z",
     "shell.execute_reply.started": "2024-02-15T05:40:23.250735Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(**kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_loss\"),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = ViT(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_loss\"], \"val\": val_result[0][\"test_loss\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:10:00.445879Z",
     "iopub.status.busy": "2024-02-15T06:10:00.445470Z",
     "iopub.status.idle": "2024-02-15T06:10:12.844993Z",
     "shell.execute_reply": "2024-02-15T06:10:12.843769Z",
     "shell.execute_reply.started": "2024-02-15T06:10:00.445848Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:15:32.508044Z",
     "iopub.status.busy": "2024-02-15T06:15:32.507573Z",
     "iopub.status.idle": "2024-02-15T06:15:32.630227Z",
     "shell.execute_reply": "2024-02-15T06:15:32.629280Z",
     "shell.execute_reply.started": "2024-02-15T06:15:32.507973Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "def load_model_for_torch_summary(**kwargs):\n",
    "    model_torch_summary = ViT(**kwargs)\n",
    "    return model_torch_summary\n",
    "\n",
    "model_torch_summary = load_model_for_torch_summary(model_kwargs=model_kwargs, lr=lr, wd=wd, beta1=beta1, beta2=beta2)\n",
    "summary(model_torch_summary.to(device), (1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T05:40:24.275425Z",
     "iopub.status.busy": "2024-02-15T05:40:24.274751Z",
     "iopub.status.idle": "2024-02-15T06:02:56.392713Z",
     "shell.execute_reply": "2024-02-15T06:02:56.391762Z",
     "shell.execute_reply.started": "2024-02-15T05:40:24.275373Z"
    }
   },
   "outputs": [],
   "source": [
    "model, results = train_model(\n",
    "    model_kwargs=model_kwargs,\n",
    "    lr=lr, wd=wd, beta1=beta1, beta2=beta2\n",
    ")\n",
    "print(\"ViT results\", results)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:03:04.767684Z",
     "iopub.status.busy": "2024-02-15T06:03:04.767264Z",
     "iopub.status.idle": "2024-02-15T06:03:10.138583Z",
     "shell.execute_reply": "2024-02-15T06:03:10.137537Z",
     "shell.execute_reply.started": "2024-02-15T06:03:04.767650Z"
    }
   },
   "outputs": [],
   "source": [
    "from train_val_test_boilerplate import test\n",
    "\n",
    "# Below values calculated during data preparation. See above.\n",
    "minimum = MIN_VALS\n",
    "maximum = MAX_VALS\n",
    "\n",
    "params_true, params_NN, errors_NN, filenames = test(model, test_loader, g=g, h=h, device=device, minimum=minimum, maximum=maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:03:13.618488Z",
     "iopub.status.busy": "2024-02-15T06:03:13.618043Z",
     "iopub.status.idle": "2024-02-15T06:03:13.622712Z",
     "shell.execute_reply": "2024-02-15T06:03:13.621777Z",
     "shell.execute_reply.started": "2024-02-15T06:03:13.618453Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n",
    "# %tensorboard --logdir ./tensorboards/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:03:17.215531Z",
     "iopub.status.busy": "2024-02-15T06:03:17.214730Z",
     "iopub.status.idle": "2024-02-15T06:04:00.139252Z",
     "shell.execute_reply": "2024-02-15T06:04:00.138148Z",
     "shell.execute_reply.started": "2024-02-15T06:03:17.215493Z"
    }
   },
   "outputs": [],
   "source": [
    "from evaluation_analysis import post_test_analysis\n",
    "\n",
    "post_test_analysis(\n",
    "    params_true, params_NN, errors_NN, filenames,\n",
    "    test_loader, params, num_sims, MEAN, STD, MEAN_DENSITIES, minimum, maximum,\n",
    "    num_maps_per_projection_direction, None, dr, channels, fmodel,\n",
    "    device=device, test_results_filename='test_results.csv', cka_filename='cka_matrix_pretrained_CNN_grid64_test.png',\n",
    "    smallest_sim_number=0, model=model.model,\n",
    "    return_layers = {\n",
    "        'transformer.0.linear.1': 'GELU',\n",
    "        'transformer.1.linear.1': 'GELU',\n",
    "        'transformer.2.linear.1': 'GELU',\n",
    "        'transformer.3.linear.1': 'GELU',\n",
    "        'transformer.4.linear.1': 'GELU',\n",
    "        'transformer.5.linear.1': 'GELU'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the ViT\n",
    "\n",
    "See [here](https://github.com/jacobgil/pytorch-grad-cam/issues/140) for more information.\n",
    "\n",
    "TODO: Grad-CAM section below is commented since the code didn't work. Try to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:23:40.716722Z",
     "iopub.status.busy": "2024-02-15T06:23:40.716043Z",
     "iopub.status.idle": "2024-02-15T06:23:40.720775Z",
     "shell.execute_reply": "2024-02-15T06:23:40.719777Z",
     "shell.execute_reply.started": "2024-02-15T06:23:40.716687Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.model.transformer[-1].layer_norm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:23:41.008351Z",
     "iopub.status.busy": "2024-02-15T06:23:41.008025Z",
     "iopub.status.idle": "2024-02-15T06:23:41.017014Z",
     "shell.execute_reply": "2024-02-15T06:23:41.016023Z",
     "shell.execute_reply.started": "2024-02-15T06:23:41.008325Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class GradCAMRegressor:\n",
    "#     def __init__(self, model, target_layer, ground_truth_param_value, index):\n",
    "#         self.model = model\n",
    "#         self.target_layer = target_layer\n",
    "#         self.feature_maps = None  # Placeholder for feature maps\n",
    "#         self.gradients = None  # Placeholder for gradients\n",
    "#         self.ground_truth_param_value = ground_truth_param_value\n",
    "#         self.index = index\n",
    "\n",
    "#         # Set the model to evaluation mode\n",
    "#         self.model.eval()\n",
    "\n",
    "#         # Register a hook to capture gradients of the target layer\n",
    "#         self.hook = self.register_hooks()\n",
    "\n",
    "#     def register_hooks(self):\n",
    "#         def forward_hook(module, input, output):\n",
    "#             self.feature_maps = output\n",
    "\n",
    "#         def backward_hook(module, grad_input, grad_output):\n",
    "#             self.gradients = grad_output[0]\n",
    "\n",
    "#         # Register hooks for both forward and backward passes\n",
    "#         forward_hook_handle = self.target_layer.register_forward_hook(forward_hook)\n",
    "#         backward_hook_handle = self.target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "#         return forward_hook_handle, backward_hook_handle\n",
    "\n",
    "#     def remove_hooks(self):\n",
    "#         # Remove hooks after usage\n",
    "#         self.hook[0].remove()\n",
    "#         self.hook[1].remove()\n",
    "\n",
    "#     def generate_gradcam(self, input_tensor, interpolate=False, target_size=(64, 64)):  # If interpolate=True, interpolates the Grad-CAM heatmap to target_size. target_size is used only when interpolate=True.\n",
    "#         # Forward pass\n",
    "#         input_tensor.requires_grad_()\n",
    "#         self.model.zero_grad()\n",
    "#         model_output = self.model(input_tensor)[:, self.index]\n",
    "\n",
    "#         D = model_output - self.ground_truth_param_value\n",
    "#         # d = 1 / D\n",
    "\n",
    "#         # Backward pass to compute gradients\n",
    "#         model_output.backward(torch.ones_like(model_output), retain_graph=True)\n",
    "\n",
    "#         # The approach is from https://arxiv.org/pdf/2304.08192.pdf\n",
    "#         # TODO: CHANGED FOR ViT BELOW LINE\n",
    "#         self.gradients = self.gradients * (-1 / (D**2))\n",
    "\n",
    "#         # Retrieve gradients and feature maps\n",
    "#         gradients = self.gradients  # Gradients of the target layer\n",
    "#         feature_maps = self.feature_maps  # Output of the target layer\n",
    "\n",
    "#         print(gradients.shape, feature_maps.shape)\n",
    "#         return\n",
    "\n",
    "#         # TODO: Doing the below to prevent error for now. Need to find reliable solution.\n",
    "#         # TODO: Why is gradients and features a 3d tensor instead of 4d? what each dimension in that represents?\n",
    "#         # TODO: I get 17 in one dim = 1+4*4. So i need to smth like x[1:] to get 16 elements, and then reshape to 4X4.\n",
    "#         gradients = gradients.unsqueeze(0)\n",
    "#         feature_maps = feature_maps.unsqueeze(0)\n",
    "\n",
    "#         print(f'Gradients shape: {gradients.shape}')\n",
    "#         print(f'Feature maps shape: {feature_maps.shape}')\n",
    "\n",
    "#         # pool the gradients across the channels\n",
    "#         pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "#         # weight the channels by corresponding gradients\n",
    "#         for i in range(feature_maps.size()[1]):\n",
    "#             feature_maps[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "#         # average the channels of the activations and squeeze across the batch dimension since we assume only one image is passed.\n",
    "#         # Note: If want to support multiple images at once, need to remove squeeze(dim=0) and instead add mean(dim=0) to average cams across batch images.\n",
    "#         cam = torch.mean(feature_maps, dim=1).squeeze(dim=0)\n",
    "\n",
    "#         # relu on top of the heatmap\n",
    "#         cam = F.relu(cam)\n",
    "\n",
    "# #         # normalize the heatmap\n",
    "# #         cam /= torch.max(cam)\n",
    "\n",
    "# #         # Calculate weighted combination of feature maps\n",
    "# #         weights = F.adaptive_avg_pool2d(gradients, 1)\n",
    "# #         cam = torch.sum(weights * feature_maps, dim=1, keepdim=True)\n",
    "# #         cam = F.relu(cam)\n",
    "        \n",
    "#         if interpolate:\n",
    "#             cam = cam.unsqueeze(0).unsqueeze(0)\n",
    "#             # Resize Grad-CAM to match the input image size\n",
    "#             cam = F.interpolate(cam, size=target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "#         return cam\n",
    "\n",
    "#     def visualize_gradcam(self, input_tensor, target_size=(64, 64)):\n",
    "#         gradcam = self.generate_gradcam(input_tensor)\n",
    "#         print(f'Shape of the raw gradcam map: {gradcam.shape}')\n",
    "\n",
    "#         gradcam = gradcam.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "#         # Resize Grad-CAM to match the input image size\n",
    "#         gradcam = F.interpolate(gradcam, size=target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "#         print(f'Shape of the interpolated gradcam map: {gradcam.shape}')\n",
    "\n",
    "#         # Convert to numpy array for visualization\n",
    "#         gradcam = gradcam.squeeze().cpu().detach().numpy()\n",
    "\n",
    "#         # Normalize for visualization\n",
    "#         gradcam = (gradcam - np.min(gradcam)) / (np.max(gradcam) - np.min(gradcam) + 1e-8)\n",
    "\n",
    "#         # Display the original image\n",
    "#         original_image = input_tensor[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "#         original_image = (original_image - np.min(original_image)) / (np.max(original_image) - np.min(original_image) + 1e-8)\n",
    "#         plt.imshow(original_image)\n",
    "\n",
    "#         # Overlay Grad-CAM on the original image\n",
    "#         plt.imshow(gradcam, cmap='jet', alpha=0.3, interpolation='bilinear')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:23:41.337198Z",
     "iopub.status.busy": "2024-02-15T06:23:41.336842Z",
     "iopub.status.idle": "2024-02-15T06:23:41.344472Z",
     "shell.execute_reply": "2024-02-15T06:23:41.343463Z",
     "shell.execute_reply.started": "2024-02-15T06:23:41.337171Z"
    }
   },
   "outputs": [],
   "source": [
    "# # from grad_cam_interpret import GradCAMRegressor\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# images, labels, _ = next(iter(test_loader))\n",
    "# images = images.to(device)\n",
    "# labels = labels.to(device)\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming 'model' is your regression model and 'target_layer' is the layer you want to visualize\n",
    "# for img_idx in [5, 7, 10, 13, 15]:\n",
    "#     # Only select one image\n",
    "#     images_ = images[img_idx, :].unsqueeze(0)\n",
    "#     print(images_.shape)\n",
    "#     for index in [0, 4]:\n",
    "#         if index == 0:\n",
    "#             param = r'$\\Omega_m$'\n",
    "#         elif index == 4:\n",
    "#             param = r'$\\sigma_8$'\n",
    "\n",
    "#         gradcam_regressor = GradCAMRegressor(model, target_layer=model.model.transformer[-1].layer_norm_1, ground_truth_param_value=labels[:, index][img_idx], index=index)\n",
    "\n",
    "#         # Visualize Grad-CAM for the entire image in a regression context\n",
    "#         gradcam = gradcam_regressor.generate_gradcam(images_)\n",
    "\n",
    "#         # Remove hooks after usage\n",
    "#         gradcam_regressor.remove_hooks()\n",
    "\n",
    "#         fig, ax = plt.subplots(2, 3, figsize=(15, 7))\n",
    "#         show_image = images_.squeeze().cpu().detach().numpy()\n",
    "#         ax[0,0].imshow(show_image)\n",
    "#         ax[0,1].imshow(gradcam.cpu().detach().numpy())\n",
    "\n",
    "#         gradcam = gradcam.unsqueeze(0).unsqueeze(0)\n",
    "#         gradcam = gradcam.squeeze().reshape(16,16).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "#         # Resize Grad-CAM to match the input image size\n",
    "#         gradcamI = F.interpolate(gradcam, size=show_image.shape, mode='bilinear', align_corners=False)\n",
    "#         # Convert to numpy array for visualization\n",
    "#         gradcamI = gradcamI.squeeze().cpu().detach().numpy()\n",
    "#         # Normalize for visualization\n",
    "#         gradcamI = (gradcamI - np.min(gradcamI)) / (np.max(gradcamI) - np.min(gradcamI) + 1e-8)\n",
    "#         original_image = images_[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "#         original_image = (original_image - np.min(original_image)) / (np.max(original_image) - np.min(original_image) + 1e-8)\n",
    "#         ax[0,2].imshow(original_image)\n",
    "#         # Overlay Grad-CAM on the original image\n",
    "#         ax[0,2].imshow(gradcamI, cmap='jet', alpha=0.3, interpolation='bilinear')\n",
    "#         ax[0,0].set_title(param)\n",
    "\n",
    "#         # Plot normalized density and gradcam value.\n",
    "#         den_gradcam_pairs = []\n",
    "#         original_image = original_image.squeeze()\n",
    "#         gradcam = gradcam.squeeze().cpu().detach().numpy()\n",
    "#         receptive_size = (original_image.shape[0]//gradcam.shape[0], original_image.shape[1]//gradcam.shape[1])\n",
    "#         for i in range(0, original_image.shape[0], original_image.shape[0]//gradcam.shape[0]):\n",
    "#             for j in range(0, original_image.shape[1], original_image.shape[1]//gradcam.shape[1]):\n",
    "#                 mean_den = original_image[i:i+receptive_size[0], j:j+receptive_size[1]].mean()\n",
    "#                 this_grad_cam = gradcam[i//receptive_size[0], j//receptive_size[1]]\n",
    "#                 den_gradcam_pairs.append((mean_den, this_grad_cam))\n",
    "\n",
    "#         ax[1,0].scatter([d[0] for d in den_gradcam_pairs], [d[1] for d in den_gradcam_pairs]);\n",
    "#         ax[1,0].set_xlabel('Normalized density')\n",
    "#         ax[1,0].set_ylabel('Normalized Grad-CAM')\n",
    "\n",
    "#         sns.kdeplot(x=original_image.flatten(), y=gradcamI.flatten(), ax=ax[1,1], fill=True)\n",
    "#         ax[1,1].set_xlabel('Normalized density')\n",
    "#         ax[1,1].set_ylabel('Interpolated normalized Grad-CAM')\n",
    "\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:23:42.160077Z",
     "iopub.status.busy": "2024-02-15T06:23:42.159331Z",
     "iopub.status.idle": "2024-02-15T06:23:42.165834Z",
     "shell.execute_reply": "2024-02-15T06:23:42.164930Z",
     "shell.execute_reply.started": "2024-02-15T06:23:42.160046Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"paper\", font_scale = 2)\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(style='ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:23:42.496433Z",
     "iopub.status.busy": "2024-02-15T06:23:42.495792Z",
     "iopub.status.idle": "2024-02-15T06:23:42.500500Z",
     "shell.execute_reply": "2024-02-15T06:23:42.499473Z",
     "shell.execute_reply.started": "2024-02-15T06:23:42.496400Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import smooth_3D_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:23:43.658052Z",
     "iopub.status.busy": "2024-02-15T06:23:43.657330Z",
     "iopub.status.idle": "2024-02-15T06:23:43.662264Z",
     "shell.execute_reply": "2024-02-15T06:23:43.661257Z",
     "shell.execute_reply.started": "2024-02-15T06:23:43.658017Z"
    }
   },
   "outputs": [],
   "source": [
    "halo_dirname = '/kaggle/input/halo-vit-lss-64-same-sims-no-mass-cuts'  # Sims 0-999. These are same simulations as the density fields (0-999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:23:45.994511Z",
     "iopub.status.busy": "2024-02-15T06:23:45.993557Z",
     "iopub.status.idle": "2024-02-15T06:23:45.998492Z",
     "shell.execute_reply": "2024-02-15T06:23:45.997536Z",
     "shell.execute_reply.started": "2024-02-15T06:23:45.994477Z"
    }
   },
   "outputs": [],
   "source": [
    "SAME_SIMS = True  # Whether the halo and DM density simulations exactly match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:24:07.660076Z",
     "iopub.status.busy": "2024-02-15T06:24:07.659404Z",
     "iopub.status.idle": "2024-02-15T06:25:23.353365Z",
     "shell.execute_reply": "2024-02-15T06:25:23.352366Z",
     "shell.execute_reply.started": "2024-02-15T06:24:07.660041Z"
    }
   },
   "outputs": [],
   "source": [
    "if SAME_SIMS:\n",
    "    # Analysis of the bias parameter\n",
    "    import h5py\n",
    "    import os\n",
    "    import glob\n",
    "    import numpy as np\n",
    "    from utils import read_hdf5\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from scipy.stats import iqr\n",
    "\n",
    "    DEN_FIELD_DIRECTORY = 'my_outputs'\n",
    "\n",
    "    if USE_COLAB:\n",
    "        dpath = os.path.join('/content/', f'{DEN_FIELD_DIRECTORY}', '*.h5')\n",
    "        hpath = os.path.join('/content/', f'{DEN_FIELD_DIRECTORY}_halo', '*.h5')\n",
    "    else:\n",
    "        dpath = os.path.join('/kaggle/input/density-fields-vit-lss-64/', f'{DEN_FIELD_DIRECTORY}', '*.h5')\n",
    "        hpath = os.path.join(f'{halo_dirname}', f'{DEN_FIELD_DIRECTORY}_halo', '*.h5')\n",
    "\n",
    "    all_bias_params = []\n",
    "    dens = []\n",
    "    halos = []\n",
    "    for i, filename in enumerate(\n",
    "        zip(\n",
    "          sorted(glob.glob(dpath)),\n",
    "          sorted(glob.glob(hpath))\n",
    "        )\n",
    "    ):\n",
    "        # print(filename[0], filename[1])\n",
    "        den, dparams = read_hdf5(filename[0], dataset_name='3D_density_field')\n",
    "        halo, hparams = read_hdf5(filename[1], dataset_name='3D_halo_distribution')\n",
    "\n",
    "        den = smooth_3D_field(den)\n",
    "        halo = smooth_3D_field(halo)\n",
    "\n",
    "        den_contrast = den/den.mean() - 1\n",
    "        halo_contrast = halo/halo.mean() - 1\n",
    "\n",
    "        dens.append(den_contrast)\n",
    "        halos.append(halo_contrast)\n",
    "    #     bias_params = (halo_contrast[np.where(den_contrast < 1)]/den_contrast[np.where(den_contrast < 1)])\n",
    "        bias_params = (halo_contrast/den_contrast)\n",
    "\n",
    "        # # Remove outliers.\n",
    "        # bias_params = bias_params[(bias_params > np.quantile(bias_params, 0.01)) & (bias_params < np.quantile(bias_params, 0.99))]\n",
    "        all_bias_params.append(np.median(bias_params))\n",
    "\n",
    "    all_bias_params = np.array(all_bias_params)\n",
    "    bias = np.mean(all_bias_params)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "    ax.hist(all_bias_params.ravel(), bins=20)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f'Bias: {bias:.2f} +/- {np.std(all_bias_params):.2f}')\n",
    "    plt.show()\n",
    "\n",
    "    # fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "    # ax.scatter(dens, halos, alpha=0.6)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:25:46.016870Z",
     "iopub.status.busy": "2024-02-15T06:25:46.016490Z",
     "iopub.status.idle": "2024-02-15T06:27:18.241891Z",
     "shell.execute_reply": "2024-02-15T06:27:18.240831Z",
     "shell.execute_reply.started": "2024-02-15T06:25:46.016838Z"
    }
   },
   "outputs": [],
   "source": [
    "if SAME_SIMS:\n",
    "    from utils import power_spectrum\n",
    "\n",
    "    # Analysis of the bias parameter\n",
    "    import h5py\n",
    "    import os\n",
    "    import glob\n",
    "    import numpy as np\n",
    "    from utils import read_hdf5\n",
    "    import matplotlib.pyplot as plt\n",
    "    import contextlib\n",
    "\n",
    "    DEN_FIELD_DIRECTORY = 'my_outputs'\n",
    "\n",
    "    if USE_COLAB:\n",
    "        dpath = os.path.join('/content/', f'{DEN_FIELD_DIRECTORY}', '*.h5')\n",
    "        hpath = os.path.join('/content/', f'{DEN_FIELD_DIRECTORY}_halo', '*.h5')\n",
    "    else:\n",
    "        dpath = os.path.join('/kaggle/input/density-fields-vit-lss-64/', f'{DEN_FIELD_DIRECTORY}', '*.h5')\n",
    "        hpath = os.path.join(f'{halo_dirname}', f'{DEN_FIELD_DIRECTORY}_halo', '*.h5')\n",
    "\n",
    "    Pk_dens = []\n",
    "    Pk_halos = []\n",
    "    for i, filename in enumerate(\n",
    "        zip(\n",
    "          sorted(glob.glob(dpath)),\n",
    "          sorted(glob.glob(hpath))\n",
    "        )\n",
    "    ):\n",
    "        # print(filename[0], filename[1])\n",
    "        den, dparams = read_hdf5(filename[0], dataset_name='3D_density_field')\n",
    "        halo, hparams = read_hdf5(filename[1], dataset_name='3D_halo_distribution')\n",
    "\n",
    "        with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):  # Prevent unnecessary verbose output from printing on screen.\n",
    "            k_den, Pk_den = power_spectrum(den, dimensional=3)\n",
    "            k_halo, Pk_halo = power_spectrum(halo, dimensional=3)\n",
    "\n",
    "        # Discard scales below the Nyquist frequency since they are unreliable.\n",
    "        L = 1000   # Mpc/h\n",
    "        Ng = 64\n",
    "        k_Nq = (2 * np.pi / L) * (Ng / 2)\n",
    "        condition_k = k_den <= k_Nq\n",
    "        k_den = k_den[condition_k]\n",
    "        k_halo = k_halo[condition_k]\n",
    "\n",
    "        # Remove shot noise from halo power spectra: Pk' = Pk - (1/n_bar)\n",
    "        # The field `halo` contains the effective no. of particles inside a 2D cell (n_bar).\n",
    "        # And Pk has units (Mpc/h)^2, so n_bar must be divided by the area of each cell.\n",
    "        n_bar = halo.mean()\n",
    "        n_bar = n_bar / ((L / Ng) ** 2)\n",
    "        Pk_halo = Pk_halo - (1 / n_bar)\n",
    "\n",
    "        # Select Pk powers corresponding to the refined set of wavenumbers.\n",
    "        Pk_den = Pk_den[condition_k]\n",
    "        Pk_halo = Pk_halo[condition_k]\n",
    "\n",
    "        Pk_dens.append(Pk_den)\n",
    "        Pk_halos.append(Pk_halo)\n",
    "\n",
    "    Pk_den = np.vstack(Pk_dens).mean(axis=0)\n",
    "    Pk_halo = np.vstack(Pk_halos).mean(axis=0)\n",
    "\n",
    "    assert np.all(k_den == k_halo)\n",
    "\n",
    "    # Most likely bias from the power spectrum.\n",
    "    # Following this paper: https://iopscience.iop.org/article/10.1088/0004-637X/724/2/878 >>> \"We calculate b2 as the average over the 10 largest wavelength modes in the simulation\".\n",
    "    most_likely_bias_from_ps = np.sqrt(np.mean(Pk_halo[:10]/Pk_den[:10]))\n",
    "\n",
    "    print(f'Most likely bias from the power spectrum: {most_likely_bias_from_ps}')\n",
    "    print(f'Bias obtained from the halo_contrast/DM_density_contrast analysis in the above cell: {bias}')\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(6, 5))\n",
    "    fig.subplots_adjust(hspace=0)\n",
    "    ax[0].loglog(k_den, Pk_den, c='black', label='DM', linewidth=4)\n",
    "    ax[0].loglog(k_den, Pk_halo, c='black', linestyle='--', label='Halo', linewidth=4);\n",
    "    ax[0].legend();\n",
    "    ax[0].set_ylabel(r'$P(k)$')\n",
    "    ax[1].loglog(k_den, np.sqrt(Pk_halo/Pk_den), linewidth=4)\n",
    "    ax[1].set_ylabel(r'$\\sqrt{P_{halo}(k)/P_{dm}(k)}$')\n",
    "    ax[1].set_xlabel(r'$k$')\n",
    "    ax[1].axhline(y=bias, linestyle='--', c='gray', linewidth=2)\n",
    "    ax[1].axhline(y=most_likely_bias_from_ps, linestyle='--', c='red', linewidth=2)\n",
    "    # ax[1].set_ylim([np.mean(all_bias_params)-3*np.std(all_bias_params), np.mean(all_bias_params)+3*np.std(all_bias_params)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the power spectrum is not the best choice since it leads to a degeneracy between bias and $\\sigma_8$. See e.g., https://arxiv.org/pdf/2006.01146.pdf\n",
    "\n",
    "QUESTION: It looks like the bias found here (~1.5-1.6) is a bit on the higher side than typically found values such as 1.15 or so (for e.g., here). Could this be because of the small grid size? Due to the small grid size, the halos may not be resolved well, and hence these estimates may not be very reliable. This is just my guess. Bias dependence on resolution was mentioned in this paper as well.\n",
    "\n",
    "Understanding any correlation between and bias. So we plot both for each simulation in the below plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:27:18.244098Z",
     "iopub.status.busy": "2024-02-15T06:27:18.243744Z",
     "iopub.status.idle": "2024-02-15T06:28:25.519442Z",
     "shell.execute_reply": "2024-02-15T06:28:25.518461Z",
     "shell.execute_reply.started": "2024-02-15T06:27:18.244068Z"
    }
   },
   "outputs": [],
   "source": [
    "if SAME_SIMS:\n",
    "    biases, sigma_8s, omegams = [], [], []\n",
    "    for i, filename in enumerate(\n",
    "        zip(\n",
    "          sorted(glob.glob(dpath)),\n",
    "          sorted(glob.glob(hpath))\n",
    "        )\n",
    "    ):\n",
    "        den, dparams = read_hdf5(filename[0], dataset_name='3D_density_field')\n",
    "        halo, hparams = read_hdf5(filename[1], dataset_name='3D_halo_distribution')\n",
    "\n",
    "        sigma_8 = dparams[-1]\n",
    "        omega_m = dparams[0]\n",
    "        assert sigma_8 == hparams[-1]\n",
    "        assert omega_m == hparams[0]\n",
    "\n",
    "        den = smooth_3D_field(den)\n",
    "        halo = smooth_3D_field(halo)\n",
    "\n",
    "        den_contrast = den/den.mean() - 1\n",
    "        halo_contrast = halo/halo.mean() - 1\n",
    "\n",
    "        bias_param = np.median(halo_contrast/den_contrast)\n",
    "\n",
    "        biases.append(bias_param)\n",
    "        sigma_8s.append(sigma_8)\n",
    "        omegams.append(omega_m)\n",
    "\n",
    "    # Fit a line to the bias vs sigma_8 data points.\n",
    "    from scipy import stats\n",
    "    res = stats.linregress(biases, sigma_8s)  # res contains slope, intercept, r_value, p_value, std_err.\n",
    "\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    im = ax.scatter(biases, sigma_8s, c=omegams, alpha=0.75)\n",
    "    ax.plot(biases, res.intercept + res.slope * np.array(biases), 'r', label='fitted line')\n",
    "    ax.set_xlabel(r'$b$')\n",
    "    ax.set_ylabel(r'$\\sigma_8$')\n",
    "    ax.set_title(r'$b$ vs $\\sigma_8$ for all 1000 simulations')\n",
    "    ax.legend()\n",
    "    print(fr'Equation of fitted line: $\\sigma_8 = {res.slope:.2f} * bias + {res.intercept:.2f}$')\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    cbar = fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "    cbar.ax.set_ylabel(r'$\\Omega_m$', rotation=270)\n",
    "    cbar.ax.get_yaxis().labelpad = 15\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    sns.kdeplot(x=biases, y=sigma_8s, ax=ax, fill=True)\n",
    "    ax.set_xlabel(r'$b$')\n",
    "    ax.set_ylabel(r'$\\sigma_8$')\n",
    "    ax.set_title(r'$b$ vs $\\sigma_8$ for all 1000 simulations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One speculation: if we use bias (=1.58) to scale the halo distribution for transfer learning, then predictions for $\\sigma_8$ corresponding to ranges corresponding to the bias value of 1.58, which from the above figure seems to be ~0.7-0.8, could be the most affected. One can look at the prediction plots below to ascertain this.\n",
    "\n",
    "Plotting $\\delta$ vs. $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T06:28:25.521248Z",
     "iopub.status.busy": "2024-02-15T06:28:25.520872Z"
    }
   },
   "outputs": [],
   "source": [
    "if SAME_SIMS:\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "    biases, dm_density_contrasts = [], []\n",
    "    counter = 0\n",
    "    for i, filename in enumerate(\n",
    "        zip(\n",
    "          sorted(glob.glob(dpath)),\n",
    "          sorted(glob.glob(hpath))\n",
    "        )\n",
    "    ):\n",
    "        den, dparams = read_hdf5(filename[0], dataset_name='3D_density_field')\n",
    "        halo, hparams = read_hdf5(filename[1], dataset_name='3D_halo_distribution')\n",
    "\n",
    "        den = smooth_3D_field(den)\n",
    "        halo = smooth_3D_field(halo)\n",
    "\n",
    "        den_contrast = den/den.mean() - 1\n",
    "        halo_contrast = halo/halo.mean() - 1\n",
    "\n",
    "        bias_param = halo_contrast/den_contrast\n",
    "\n",
    "        # Remove extreme (and probably unphysical) bias parameters.\n",
    "        lower_limit_condition = bias_param > np.quantile(bias_param, 0.01)\n",
    "        upper_limit_condition = bias_param < np.quantile(bias_param, 0.99)\n",
    "\n",
    "        bias_param = bias_param[(lower_limit_condition) & (upper_limit_condition)]\n",
    "        # Need to also do this for corresponding density contrast.\n",
    "        den_contrast = den_contrast[(lower_limit_condition) & (upper_limit_condition)]\n",
    "\n",
    "        bpf = bias_param.flatten()\n",
    "        dcf = den_contrast.flatten()\n",
    "        biases.append(bpf)\n",
    "        dm_density_contrasts.append(dcf)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        im = ax[0].scatter(dcf, bpf)\n",
    "        ax[0].set_xlabel(r'$\\delta_{DM}$')\n",
    "        ax[0].set_ylabel(r'$b$')\n",
    "        ax[0].set_title(r'$\\delta_{DM}$ vs $b$')\n",
    "        ax[0].set_ylim([0, 3])\n",
    "    #     divider = make_axes_locatable(ax)\n",
    "    #     cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    #     fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "    #     sns.kdeplot(x=den_contrast.flatten(), y=bias_param.flatten(), ax=ax[1], fill=True)\n",
    "        ax[1].hexbin(dcf, bpf)\n",
    "        ax[1].set_xlabel(r'$\\delta_{DM}$')\n",
    "        ax[1].set_ylabel(r'$b$')\n",
    "        ax[1].set_title(r'$\\delta_{DM}$ vs $b$')\n",
    "        ax[1].set_ylim([0, 3])\n",
    "        plt.show()\n",
    "\n",
    "    #     fig, ax = plt.subplots(1, 1)\n",
    "    #     sns.kdeplot(x=dm_density_contrasts, y=biases, ax=ax, fill=True)\n",
    "    #     ax.set_xlabel(r'$\\delta_{DM}$')\n",
    "    #     ax.set_ylabel(r'$b$')\n",
    "    #     ax.set_title(r'$\\delta_{DM}$ vs $b$ for all 1000 simulations')\n",
    "    #     ax.set_ylim([0, 2])\n",
    "    #     plt.show()\n",
    "\n",
    "        counter += 1\n",
    "        if counter == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the PDF of density and halo distribution for comparison.\n",
    "\n",
    "In the below plot, we use the mean DM density for calculating the density contrast and mean halo density for the halo contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAME_SIMS:\n",
    "    dens, halos = [], []\n",
    "    for i, filename in enumerate(\n",
    "        zip(\n",
    "          sorted(glob.glob(dpath)),\n",
    "          sorted(glob.glob(hpath))\n",
    "        )\n",
    "    ):\n",
    "        den, dparams = read_hdf5(filename[0], dataset_name='3D_density_field')\n",
    "        halo, hparams = read_hdf5(filename[1], dataset_name='3D_halo_distribution')\n",
    "\n",
    "        den = smooth_3D_field(den)\n",
    "        halo = smooth_3D_field(halo)\n",
    "\n",
    "        dens.append(den/den.mean())\n",
    "        halos.append(halo/halo.mean())\n",
    "\n",
    "    dens = np.array(dens)\n",
    "    halos = np.array(halos)\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.hist(dens.ravel(), histtype='step', linewidth=3, label='DM density')\n",
    "    ax.hist(halos.ravel(), histtype='step', linewidth=3, label='Halo')\n",
    "    ax.set_xlabel(r'$1 + \\delta$')\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: use precomputed_mean, precomputed_stddev, precomputed_min_vals, and precomputed_max_vals only if you are using bias.\n",
    "# Else it's better to use statistics of this new dataset for preprocessing.\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "# The logic is that when the exact same sims are used for DM density and halo, we use the bias and also\n",
    "# the precomputed statistics. Whereas if different simulations are used for DM and halo, we don't use the\n",
    "# bias and also don't use the precomputed statistics. When same sims, the bias adds a positive value, thus\n",
    "# log10(halo) does not give divide by zero error. When different sims, the same is handled by log(1+halo).\n",
    "\n",
    "#########################################################################################################\n",
    "if USE_COLAB:\n",
    "    if SAME_SIMS:\n",
    "        !python create_data.py --num_sims {num_sims} --train_frac 0.8 --test_frac 0.1 --seed 42 --path /content/my_outputs_halo --grid_size 64 \\\n",
    "        --num_maps_per_projection_direction 10 --prefix 'halos' --dataset_name '3D_halo_distribution' --bias {bias} \\\n",
    "        --precomputed_mean {MEAN} --precomputed_stddev {STD} \\\n",
    "        --precomputed_min_vals {MIN_VALS[0]} {MIN_VALS[1]} {MIN_VALS[2]} {MIN_VALS[3]} {MIN_VALS[4]} \\\n",
    "        --precomputed_max_vals {MAX_VALS[0]} {MAX_VALS[1]} {MAX_VALS[2]} {MAX_VALS[3]} {MAX_VALS[4]} \\\n",
    "        --smallest_sim_number 0\n",
    "    else:  # don't use bias.\n",
    "        !python create_data.py --num_sims {num_sims} --train_frac 0.8 --test_frac 0.1 --seed 42 --path /content/my_outputs_halo --grid_size 64 \\\n",
    "        --num_maps_per_projection_direction 10 --prefix 'halos' --dataset_name '3D_halo_distribution' \\\n",
    "        --smallest_sim_number 1000 --log_1_plus\n",
    "#         --precomputed_mean {MEAN} --precomputed_stddev {STD} \\\n",
    "#         --precomputed_min_vals {MIN_VALS[0]} {MIN_VALS[1]} {MIN_VALS[2]} {MIN_VALS[3]} {MIN_VALS[4]} \\\n",
    "#         --precomputed_max_vals {MAX_VALS[0]} {MAX_VALS[1]} {MAX_VALS[2]} {MAX_VALS[3]} {MAX_VALS[4]} \\\n",
    "else:\n",
    "    if SAME_SIMS:\n",
    "        !python create_data.py --num_sims {num_sims} --train_frac 0.8 --test_frac 0.1 --seed 42 --path {halo_dirname}/my_outputs_halo --grid_size 64 \\\n",
    "        --num_maps_per_projection_direction 10 --prefix 'halos' --dataset_name '3D_halo_distribution' --bias {bias} \\\n",
    "        --precomputed_mean {MEAN} --precomputed_stddev {STD} \\\n",
    "        --precomputed_min_vals {MIN_VALS[0]} {MIN_VALS[1]} {MIN_VALS[2]} {MIN_VALS[3]} {MIN_VALS[4]} \\\n",
    "        --precomputed_max_vals {MAX_VALS[0]} {MAX_VALS[1]} {MAX_VALS[2]} {MAX_VALS[3]} {MAX_VALS[4]} \\\n",
    "        --smallest_sim_number 0\n",
    "    else:  # don't use bias.'\n",
    "        !python create_data.py --num_sims {num_sims} --train_frac 0.8 --test_frac 0.1 --seed 42 --path {halo_dirname}/my_outputs_halo --grid_size 64 \\\n",
    "        --num_maps_per_projection_direction 10 --prefix 'halos' --dataset_name '3D_halo_distribution' \\\n",
    "        --smallest_sim_number 1000 --log_1_plus\n",
    "#         --precomputed_mean {MEAN} --precomputed_stddev {STD} \\\n",
    "#         --precomputed_min_vals {MIN_VALS[0]} {MIN_VALS[1]} {MIN_VALS[2]} {MIN_VALS[3]} {MIN_VALS[4]} \\\n",
    "#         --precomputed_max_vals {MAX_VALS[0]} {MAX_VALS[1]} {MAX_VALS[2]} {MAX_VALS[3]} {MAX_VALS[4]} \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the mean, std, min_vals and max_vals into variables\n",
    "prefix = 'halos'\n",
    "MEAN = np.load(f'{prefix}_dataset_mean.npy')\n",
    "STD = np.load(f'{prefix}_dataset_std.npy')\n",
    "MIN_VALS = np.load(f'{prefix}_dataset_min_vals.npy')\n",
    "MAX_VALS = np.load(f'{prefix}_dataset_max_vals.npy')\n",
    "MEAN_DENSITIES = np.load(f'{prefix}_dataset_mean_densities.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import glob\n",
    "filename = sorted(glob.glob(f'{base_dir}/train/processed_sim*_X1_LH_z0_grid64_masCIC.npy.gz'))[0]\n",
    "f = gzip.GzipFile(filename, 'r'); halo = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train/train_original_params.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "v = df[df['0'] == '/'.join(filename.split('/')[-2:])]\n",
    "params = list(v[v.columns[-5:]].iloc[0])\n",
    "\n",
    "plt.imshow(halo); plt.title(np.round(params, 4)); plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model_dataset import CustomImageDataset\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import transforms\n",
    "transform = v2.Compose([\n",
    "    MyRotationTransform(angles=[90, 180, 270]),\n",
    "    v2.ToDtype(torch.float32)\n",
    "])\n",
    "\n",
    "train_dataset = CustomImageDataset('/content/train', normalized_cosmo_params_path='train/train_normalized_params.csv', transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomImageDataset('/content/val', normalized_cosmo_params_path='val/val_normalized_params.csv', transform=None)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = CustomImageDataset('/content/test', normalized_cosmo_params_path='test/test_normalized_params.csv', transform=None)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated parameters for the transfer learning come here.\n",
    "epochs = 10  # Smaller no. of epochs than pretraining.\n",
    "# dr = dr * 2\n",
    "# wd = wd * 2\n",
    "# lr = lr * 0.1\n",
    "\n",
    "# output files names\n",
    "floss  = 'loss_transfer_learning_halo_grid64.txt'   #file with the training and validation losses for each epoch\n",
    "fmodel = 'weights_transfer_learning_halo_grid64.pt' #file containing the weights of the best-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, wd, dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FREEZE_LAYERS = False  # Whether to freeze all layers for transfer learning. If False, all layers are retrained on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_intermediate_layer_getter import IntermediateLayerGetter as MidGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_FineTune(ViT):\n",
    "    def __init__(self, pretrained_filename, model_kwargs, lr, wd, beta1, beta2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = self.load_from_checkpoint(pretrained_filename)\n",
    "\n",
    "        # Add a new classification layer to the model\n",
    "        # TODO: When I do this, need to check if weights of self.model.mlp_head are randomized or kept pretrained.\n",
    "        self.model.FC_final = nn.Linear(len(params)*2, len(params)*2)\n",
    "        self.model.mlp_head = nn.Sequential(\n",
    "            self.model.mlp_head,\n",
    "            self.model.FC_final\n",
    "        )\n",
    "        # TODO: Print the model and ensure FC_final is not coming twice in the model, for example.\n",
    "\n",
    "        self.example_input_array = next(iter(train_loader))[0]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {\"params\": self.parameters(), \"lr\": 1e-4},\n",
    "                {\"params\": self.model.FC_final.parameters(), \"lr\": 1e-2},\n",
    "            ],\n",
    "            weight_decay=self.hparams.wd, betas=(self.hparams.beta1, self.hparams.beta2)\n",
    "        )\n",
    "        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=5)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\"\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(pretrained_filename, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"val_loss\"),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "#     pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = ViT_FineTune.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "        # After loading the pretrained model, finetune it.\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = ViT_FineTune.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_loss\"], \"val\": val_result[0][\"test_loss\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
    "model, results = finetune_model(\n",
    "    pretrained_filename, model_kwargs=model_kwargs,\n",
    "    lr=lr, wd=wd, beta1=beta1, beta2=beta2\n",
    ")\n",
    "print(\"ViT_FineTune results\", results)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_val_test_boilerplate import test\n",
    "\n",
    "# Below values calculated during data preparation. See above.\n",
    "minimum = MIN_VALS\n",
    "maximum = MAX_VALS\n",
    "\n",
    "params_true, params_NN, errors_NN, filenames = test(model, test_loader, g=g, h=h, device=device, minimum=minimum, maximum=maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_analysis import post_test_analysis\n",
    "\n",
    "post_test_analysis(\n",
    "    params_true, params_NN, errors_NN, filenames,\n",
    "    test_loader, params, num_sims, MEAN, STD, MEAN_DENSITIES, minimum, maximum,\n",
    "    num_maps_per_projection_direction, None, dr, channels, fmodel,\n",
    "    device=device, test_results_filename='test_results_transfer_learning_ViT.csv', cka_filename='cka_matrix_transfer_learning_halo_ViT_grid64_test.png',\n",
    "    smallest_sim_number=0, model=model.model,\n",
    "    return_layers = {\n",
    "        'transformer.0.linear.1': 'GELU',\n",
    "        'transformer.1.linear.1': 'GELU',\n",
    "        'transformer.2.linear.1': 'GELU',\n",
    "        'transformer.3.linear.1': 'GELU',\n",
    "        'transformer.4.linear.1': 'GELU',\n",
    "        'transformer.5.linear.1': 'GELU'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on transfer learning data FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is training from scratch, don't use the --bias option since we assume we don't have information of DM density.\n",
    "if USE_COLAB:\n",
    "    !python create_data.py --num_sims {num_sims} --train_frac 0.8 --test_frac 0.1 --seed 42 --path /content/my_outputs_halo --grid_size 64 \\\n",
    "                        --num_maps_per_projection_direction 10 --prefix 'halos' --dataset_name '3D_halo_distribution' --log_1_plus --smallest_sim_number 0\n",
    "else:\n",
    "    !python create_data.py --num_sims {num_sims} --train_frac 0.8 --test_frac 0.1 --seed 42 --path {halo_dirname}/my_outputs_halo --grid_size 64 \\\n",
    "                        --num_maps_per_projection_direction 10 --prefix 'halos' --dataset_name '3D_halo_distribution' --log_1_plus --smallest_sim_number 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model_dataset import CustomImageDataset\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import transforms\n",
    "transform = v2.Compose([\n",
    "    MyRotationTransform(angles=[90, 180, 270]),\n",
    "    v2.ToDtype(torch.float32)#, scale=False),\n",
    "])\n",
    "\n",
    "train_dataset = CustomImageDataset('/content/train', normalized_cosmo_params_path='train/train_normalized_params.csv', transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomImageDataset('/content/val', normalized_cosmo_params_path='val/val_normalized_params.csv', transform=None)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = CustomImageDataset('/content/test', normalized_cosmo_params_path='test/test_normalized_params.csv', transform=None)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I don't think floss and fmodel is used anywhere, right? If so, remove them from notebook entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated parameters for the transfer learning come here.\n",
    "epochs = 25\n",
    "# dr = dr * 2\n",
    "# wd = wd * 2\n",
    "# lr = lr * 0.1\n",
    "\n",
    "# output files names\n",
    "floss  = 'loss_transfer_learning_data_train_from_scratch_halo_grid64.txt'   #file with the training and validation losses for each epoch\n",
    "fmodel = 'weights_transfer_learning_data_train_from_scratch_halo_grid64.pt' #file containing the weights of the best-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, results = train_model(\n",
    "    model_kwargs=model_kwargs,\n",
    "    lr=lr, wd=wd, beta1=beta1, beta2=beta2\n",
    ")\n",
    "print(\"ViT results\", results)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_val_test_boilerplate import test\n",
    "\n",
    "# Below values calculated during data preparation. See above.\n",
    "minimum = MIN_VALS\n",
    "maximum = MAX_VALS\n",
    "\n",
    "params_true, params_NN, errors_NN, filenames = test(model, test_loader, g=g, h=h, device=device, minimum=minimum, maximum=maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_analysis import post_test_analysis\n",
    "\n",
    "post_test_analysis(\n",
    "    params_true, params_NN, errors_NN, filenames,\n",
    "    test_loader, params, num_sims, MEAN, STD, MEAN_DENSITIES, minimum, maximum,\n",
    "    num_maps_per_projection_direction, None, dr, channels, fmodel,\n",
    "    device=device, test_results_filename='test_results_transfer_learning_from_scratch_ViT.csv', cka_filename='cka_matrix_transfer_learning_from_scratch_halo_ViT_grid64_test.png',\n",
    "    smallest_sim_number=0, model=model.model,\n",
    "    return_layers = {\n",
    "        'transformer.0.linear.1': 'GELU',\n",
    "        'transformer.1.linear.1': 'GELU',\n",
    "        'transformer.2.linear.1': 'GELU',\n",
    "        'transformer.3.linear.1': 'GELU',\n",
    "        'transformer.4.linear.1': 'GELU',\n",
    "        'transformer.5.linear.1': 'GELU'\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3911348,
     "sourceId": 6798620,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4330737,
     "sourceId": 7440715,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
